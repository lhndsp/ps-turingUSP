---
title: "Random Forests (Florestas Aleat√≥rias)"
output: html_document
mainfont: Times New Roman
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


<center>
<img src="imagens/forest.jpg" width="400" align="center">
</center>



Floresta aleat√≥ria √© um m√©todo de aprendizado de m√°quina pertencente a classe de m√©todos de aprendizado conjunto (ensemble learning methods) pois n√£o √© um modelo unit√°rio, mas uma combina√ß√£o de outros modelos. 

Mas, antes de falarmos de florestas aleat√≥rias em si ...

<center>
<img src="imagens/calm_down.jpg" width="300" align="center">
</center>


Vamos falar brevemente do modelo que d√° origem a uma floresta aleat√≥ria...as arvores de decis√£o (ou decision trees).

#### __√Årvores de Decis√£o__

Vamos imaginar um exemplo, uma pessoa que gosta de jogar futebol ao ar livre e, ao longo do tempo foi observado que o comparecimento dessa pessoa no jogo depende de uma s√©rie de fatores associados ao clima, como temperatura, umidade, chuva, vento, etc. Podemos representar isso com a seguinte estrutura: 

<center>
<img src="imagens/esquema_arvore_simples.jpg" width="400" align="center">
</center>


Esse e um exemplo relativamente rudimentar, mas que exemplifica muito bem arvores de decis√£o, mas s√≥ para colocar um pouco de formalidade nos termos üòä: Os pontos em que √© tomada uma decis√£o entre qual caminho seguir s√£o os n√≥s (nas caixas violeta), a conex√£o entre esses nos s√£o os ramos (nas setas azul marinho, com o titulo do ramo nas caixas da mesma cor), o n√≥ inicial que inicia o processo de divis√£o √© a raiz (Clima), os √∫ltimos nos em ramos s√£o as folhas (nas caixas em laranja) e por fim a dist√¢ncia entre as folhas e a raiz √© a altura da arvore (legal n√©?).  

Observemos que intuitivamente j√° vemos o funcionamento b√°sico de uma arvore, a partir de um conjunto de informa√ß√µes, vamos tomando decis√µes a partir de par√¢metros que descrevem esse conjunto e por fim chegamos em uma resposta (ou classifica√ß√£o) do que queremos responder. 

Mas vamos incrementar um pouco as coisas e imaginar que n√£o queremos responder se apenas uma pessoa especifica vai ao jogo, mas sim queremos um modelo que responda isso para diversas pessoas que est√£o em condi√ß√µes clim√°ticas diferentes, e, que o nosso modelo de arvore agora √© um pouco mais elaborado, como: 

<center>
<figure>
<img src="imagens/esquema_arvore_maior.jpg" width="600" align="center">
    <figcaption></figcaption>
</figure>
</center>


Sim, um pouco exagerado, mas, agora que os n√≥s em que decidimos qual ramo seguir ficam t√£o espec√≠ficos, conforme a altura da arvore aumentou, que acabamos ajustando nosso modelo para uma situa√ß√£o par locais muito espec√≠ficos, e com isso acabamos falhando no objetivo que era um modelo gen√©rico üòï. 

Ent√£o, como corrigir isso? 

Bom existem diversas maneiras, mas vamos seguir por uma delas, e, imaginar um modelo que n√£o se baseasse em uma √∫nica arvore, mas v√°rias, em que cada uma vai ser parametrizada com dados (n√≥s) aleat√≥rios que dividem bem nosso conjunto de dados, ou seja, n√£o vamos ter a classifica√ß√£o baseada em uma √∫nica arvores, mas um conjunto delas, ou ... uma floresta üòÄ. 

<center>
<img src="imagens/tcharam.jpg" width="300" align="center">
</center>


#### __Florestas Aleat√≥rias__

Finalmente chegamos ent√£o ao modelo de florestas aleat√≥rias, que assim como arvores de decis√£o pode ser usado para classifica√ß√£o ou regress√£o, e, para criarmos um modelo assim, uma das maneiras √© quebrar nosso conjunto de dados em v√°rios conjuntos menores, e, a partir deles criar uma s√©rie de arvores de decis√£o distintas, e a resposta desse modelo √© geralmente a m√©dia das respostas das arvores que o comp√µe.  

E para reduzirmos o ajuste excessivo dos dados a uma √∫nica situa√ß√£o, na cria√ß√£o das arvores de decis√£o usamos um m√©todo estoc√°stico (uma palavra bonita pra aleat√≥rio üòä) na escolha dos par√¢metros que formam os n√≥s, ou seja, escolhemos aleatoriamente, mas de maneira que os dados ficam bem divididos (para isso tem algumas fun√ß√µes matem√°ticas que conduzem esse processo), os pontos em que tomamos a decis√£o de qual ramo seguir. 

# Refer√™ncias:
- Artigos Web:
    * [Turing Talks - Modelos de Predi√ß√£o: Random Forest](https://medium.com/turing-talks/turing-talks-18-modelos-de-predi%C3%A7%C3%A3o-random-forest-cfc91cd8e524)
    * [Turing Talks - Modelos de Predi√ß√£o: Introdu√ß√£o √† Predi√ß√£o](https://medium.com/turing-talks/turing-talks-10-introdu%C3%A7%C3%A3o-%C3%A0-predi%C3%A7%C3%A3o-a75cd61c268d)
    * [Turing Talks - Modelos de Predi√ß√£o: Decision Tree](https://medium.com/turing-talks/turing-talks-17-modelos-de-predi%C3%A7%C3%A3o-decision-tree-610aa484cb05)
    
    
- Livro: 
    * James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning (1st ed.) [PDF]. Springer. Cap. 8 (Tree-Based Methods)
 
 
- Fonte das imagens:
    * [Figura 1](https://unsplash.com/s/photos/tree)
    * [Figura 2](https://dontcallmejeh.tumblr.com/)
    * Figuras 3 e 4 eu criei no canvas baseado no modelo do livro citado acima
    * [Figura 5](https://aminoapps.com/c/armyaminobr/page/blog/tcharam/lla3_BkSQugnbL2ZldBmQXD21gvLLG1m0K)
